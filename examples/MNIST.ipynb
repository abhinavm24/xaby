{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on MNIST with XABY\n",
    "\n",
    "This notebook demonstrates how to train a fully connected network (not convolutional!) on MNIST with the XABY framework. I'll also compare it to PyTorch so you can see the different APIs and performances.\n",
    "\n",
    "I'm going to use torchvision to load in the MNIST data, because it's super great."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import xaby\n",
    "import xaby.nn as xn\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "mnist_train = datasets.MNIST(\"~/.pytorch\", train=True, transform=transform, download=True)\n",
    "mnist_test = datasets.MNIST(\"~/.pytorch\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=128, shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(mnist_test, batch_size=128, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Models\n",
    "\n",
    "First up, I'll define two models with the same architecture. One with XABY, the other with PyTorch. \n",
    "\n",
    "XABY models are defined as a sequence of operations. When a model is defined, it is compiled behind the scenes into a single function. You call the function with some input like `inputs >> model`. I had a lot of fun messing with Python operators. My intention of doing it this way is if you chain a lot of functions, the last function called is the first function you read. I'm using the `>>` operator so you can write the chain of functions in the order they are called.\n",
    "\n",
    "You can define the PyTorch model with `torch.nn.Sequential`, but sublassing from `torch.nn.Module` is the preferred method, so I'll do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## XABY model ##\n",
    "xaby_model = xaby.flatten >> xn.linear(784, 256) >> xn.relu \\\n",
    "                          >> xn.linear(256, 10) >> xn.log_softmax\n",
    "\n",
    "## PyTorch Model ##\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.flatten(1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.log_softmax(self.fc2(x), dim=1)\n",
    "        return x\n",
    "    \n",
    "torch_model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's run data through the models!\n",
    "\n",
    "Just a small example of using XABY models for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data from the image loader\n",
    "images, labels = next(iter(train_loader))\n",
    "\n",
    "# Convert PyTorch Tensor to a XABY Tensor\n",
    "inputs = xaby.tensor(images)\n",
    "\n",
    "# Thanks to JAX, XABY tensors are automatically on the GPU\n",
    "print(f\"XABY device: {inputs.device}\")\n",
    "\n",
    "# Call the model in a fun manner\n",
    "log_p = inputs >> xaby_model\n",
    "\n",
    "# Normal function call... boring....\n",
    "log_p = xaby_model(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I should also note you can run XABY tensors through operations without creating models. This returns another tensor. If you start the sequence with an operation, it'll create a model. If you start with a tensor, it'll run through the operations and return a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs >> xaby.flatten >> xn.linear(784, 10) >> xn.log_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing XABY and PyTorch\n",
    "\n",
    "Below I'll test how long it takes for inference with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First on CPU\n",
    "torch_model = torch_model.requires_grad_(False)\n",
    "torch_model.to(\"cpu\")\n",
    "images = images.to(\"cpu\")\n",
    "\n",
    "%timeit -n 1000 torch_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now on GPU\n",
    "torch_model.to(\"cuda\")\n",
    "images = images.to(\"cuda\")\n",
    "\n",
    "%timeit -n 1000 torch_model(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the XABY model, runs on GPU!\n",
    "inputs = xaby.tensor(images.to(\"cpu\"))\n",
    "\n",
    "%timeit -n 1000 inputs >> xaby_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XABY is slightly slower than PyTorch on the GPU. This might be due to JAX being slower or it's possible I can do some more optimization in XABY.\n",
    "\n",
    "Either way, time to train the models. First up, XABY. I'll use simple stochastic gradient descent for both. The output of the models is log-softmax, so I'll use the negative log-likelihood loss.\n",
    "\n",
    "In XABY, we create a `backprop` function that takes the input and targets, then returns the loss and gradients. When only evaluating, such as in validation, you can get the loss directly:\n",
    "```python\n",
    "loss = inputs >> model >> nlloss << targets\n",
    "```\n",
    "\n",
    "We also create an `update` function that updates a model given gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = train_loader.batch_size\n",
    "print_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.818  Test loss: 1.809  Test acc.: 0.662  Images/sec: 3054.412\n",
      "Train loss: 1.380  Test loss: 1.351  Test acc.: 0.750  Images/sec: 6638.921\n",
      "Train loss: 1.015  Test loss: 1.034  Test acc.: 0.804  Images/sec: 5866.025\n",
      "Train loss: 0.920  Test loss: 0.837  Test acc.: 0.833  Images/sec: 6021.412\n"
     ]
    }
   ],
   "source": [
    "### Define a fresh model, in two lines for readability\n",
    "model = xaby.flatten >> xn.linear(784, 256) >> xn.relu \\\n",
    "                   >> xn.linear(256, 10) >> xn.log_softmax\n",
    "\n",
    "# Model with backpropagation\n",
    "backprop = model << xn.nlloss\n",
    "\n",
    "# Update function\n",
    "update = xaby.optim.sgd(model, lr=0.003)\n",
    "\n",
    "step = 0\n",
    "start = time.time()\n",
    "for images, labels in train_loader:\n",
    "    step += 1\n",
    "    \n",
    "    inputs, targets = xaby.tensor(images), xaby.tensor(labels)\n",
    "    \n",
    "    # Backprop to get gradients!\n",
    "    train_loss, grads = backprop(inputs, targets)\n",
    "    # Update model parameters with gradients\n",
    "    grads >> update\n",
    "    \n",
    "    if step % print_every == 0:\n",
    "        stop = time.time()\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        for images, labels in test_loader:\n",
    "            inputs, targets = xaby.tensor(images), xaby.tensor(labels)\n",
    "            \n",
    "            log_p = inputs >> model\n",
    "            loss = log_p >> xn.nlloss << targets\n",
    "            accuracy = xaby.metrics.accuracy(log_p, targets)\n",
    "            \n",
    "            test_losses.append(loss.numpy())\n",
    "            test_accuracy.append(accuracy)\n",
    "            \n",
    "        print(f\"Train loss: {train_loss:.3f}  \"\n",
    "              f\"Test loss: {sum(test_losses)/len(test_losses):.3f}  \"\n",
    "              f\"Test acc.: {sum(test_accuracy)/len(test_accuracy):.3f}  \"\n",
    "              f\"Images/sec: {print_every*batch_size/(stop - start):.3f}\")\n",
    "        start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a fresh model\n",
    "torch_model = torch.nn.Sequential(\n",
    "                    torch.nn.Flatten(),\n",
    "                    torch.nn.Linear(784, 256),\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.Linear(256, 10),\n",
    "                    torch.nn.LogSoftmax(1))\n",
    "torch_model.to(\"cuda\")\n",
    "optimizer = torch.optim.SGD(torch_model.parameters(), lr=0.003)\n",
    "criterion = torch.nn.NLLLoss()\n",
    "\n",
    "step = 0\n",
    "start = time.time()\n",
    "for images, labels in train_loader:\n",
    "    step += 1\n",
    "    \n",
    "    inputs, targets = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    log_p = torch_model(inputs)\n",
    "    loss = criterion(log_p, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_loss = loss.item()\n",
    "    \n",
    "    if step % print_every == 0:\n",
    "        stop = time.time()\n",
    "        test_losses = []\n",
    "        test_accuracy = []\n",
    "        for images, labels in test_loader:\n",
    "            with torch.no_grad():\n",
    "                inputs, targets = images.to(\"cuda\"), labels.to(\"cuda\")\n",
    "                log_p = torch_model(inputs)\n",
    "                loss = criterion(log_p, targets)\n",
    "                accuracy = (log_p.argmax(axis=1) == targets).sum()/float(len(images))\n",
    "            \n",
    "            test_losses.append(loss.item())\n",
    "            test_accuracy.append(accuracy.item())\n",
    "            \n",
    "        print(f\"Train loss: {train_loss:.3f}  \"\n",
    "              f\"Test loss: {sum(test_losses)/len(test_losses):.3f}  \"\n",
    "              f\"Test accuracy: {sum(test_accuracy)/len(test_accuracy):.3f}  \"\n",
    "              f\"Images/sec: {print_every*batch_size/(stop - start):.3f}\")\n",
    "        start = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
