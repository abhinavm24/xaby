{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from jax import random\n",
    "import jax.numpy as np\n",
    "from jax import grad, jit, vjp, vmap\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import xaby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/iris.data', \n",
    "                   names=['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species'])\n",
    "\n",
    "labels = data['species'].unique().tolist()\n",
    "x = data.iloc[:, :4].values\n",
    "x = (x - x.mean(axis=0))/x.std(axis=0) # normalize\n",
    "y = data['species'].replace(labels, [0, 1, 2]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly shuffle and split into train/test\n",
    "split = 25\n",
    "key = xaby.random.key_manager.key\n",
    "combined_data = np.hstack((x, y[:, None]))\n",
    "shuffled = random.shuffle(key, combined_data)\n",
    "train_x, test_x = shuffled[:-split, :4], shuffled[-split:, :4]\n",
    "train_y, test_y = shuffled[:-split, 4].astype(np.int8), shuffled[-split:, 4].astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xaby import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# Data tensors, x and y were generated above\n",
    "inputs = xaby.Tensor(train_x)\n",
    "targets = xaby.Tensor(train_y)\n",
    "test_inputs = xaby.Tensor(test_x)\n",
    "test_targets = xaby.Tensor(test_y)\n",
    "\n",
    "# Optimize with Stochastic Gradient Descent\n",
    "optimize = xaby.optim.SGD(lr=0.0003)\n",
    "\n",
    "# Define model\n",
    "model = nn.linear(4, 3) >> xaby.log_softmax\n",
    "\n",
    "# Backpropagate with Negative Log-Likelihood loss\n",
    "backprop = model << xaby.losses.nlloss\n",
    "\n",
    "# Backprop and update network\n",
    "for i in range(200):\n",
    "    loss, grads = inputs >> backprop << targets\n",
    "    model >> optimize << grads\n",
    "    \n",
    "    train_losses.append(loss/len(inputs))\n",
    "    test_loss = test_inputs >> model >> xaby.losses.nlloss << test_targets\n",
    "    test_losses.append(test_loss.item()/len(test_inputs))\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(loss, test_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.argmax((test_inputs >> net).data, axis=1)\n",
    "accuracy = (predictions == test_targets.data).mean()\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp((test_inputs >> net).data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
